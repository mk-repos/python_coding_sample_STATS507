{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6: PyTorch and Pandas\n",
    "\n",
    "name: Moeki Kurita\n",
    "\n",
    "email: mkurita@umich.edu\n",
    "\n",
    "I discussed problem 1 with Hisamitsu Maeda.\n",
    "\n",
    "This homework assignment took me 20 hours in total to complete.\n",
    "\n",
    "### Submission Instructions\n",
    "Your homework solutions should be written entirely in this Jupyter notebook file. Once it contains your solutions, you should submit this notebook through Canvas. Before submitting, please make sure make sure __Cells->Run__ executes without errors; errors in your code cause problems for our auto-grader. Thus, even though we frequently ask you to check for errors in your functions, you should not include in your submission any examples of your functions actually raising those errors.\n",
    "\n",
    "### Coding style\n",
    "Please follow the [PEP8 guidelines](https://www.python.org/dev/peps/pep-0008/). The `pycodestyle_magic` package will remind you if you are not. To install it, run\n",
    "```\n",
    "pip install flake8 pycodestyle_magic\n",
    "```\n",
    "Once it is installed the following two Jupyter directives will enable syntax checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supplemental Data Files\n",
    "Run the following code to download the supplemental files you will need to complete this homework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from os import path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "url = \"https://regier.stat.lsa.umich.edu/stats507/\"\n",
    "fn = \"hw6data.zip\"\n",
    "if not path.exists(fn):\n",
    "    data = request.urlopen(url + fn).read()\n",
    "    with open(fn, 'wb') as fhand:\n",
    "        fhand.write(data)\n",
    "    with ZipFile(fn, 'r') as zipObj:\n",
    "        zipObj.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Building and training simple models with PyTorch (10 points)\n",
    "In this problem, you'll use PyTorch to build the loss functions for a pair of commonly-used statistical models. In all cases, your answer should include variables `x` and `ytrue`, which will serve as the predictor (independent variable) and response (dependent variable), respectively. Please use `W` to denote a parameter that multiplies the predictor, and $b$ to denote a bias parameter (i.e., a parameter that is added)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Logistic regression with a negative log-likelihood loss.__ In this model, which we discussed briefly in class, the binary variable $Y$ is distributed as a Bernoulli random variable with success parameter $\\sigma(W^T X + b)$, where $\\sigma(z) = (1+\\exp(-z))^{-1}$ is the logistic function, and $X \\in R^6$ is the predictor random variable, and $W \\in R^6, b \\in R$ are the model parameters. Derive the log-likelihood of $Y$, and write the PyTorch code that represents the negative log-likelihood loss function. __Hint:__ the loss should be a negative log-likelihood term, summed over all the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. of Predictors\n",
    "p = 6\n",
    "# Initialize Params\n",
    "torch.manual_seed(1)\n",
    "w = torch.rand(p, 1, dtype=torch.float32, requires_grad=True)\n",
    "b = torch.rand(1, 1, dtype=torch.float32, requires_grad=True)\n",
    "# Memo\n",
    "# x: n*p matrix\n",
    "# y: n*1 vector, true values\n",
    "# yhat: n*1 vector, fitted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss(x, w, b, ytrue):\n",
    "    sigmoid = torch.sigmoid(torch.mm(x, w) + b)\n",
    "    los_1 = torch.mm(-ytrue.view(1, -1), sigmoid.log())\n",
    "    los_2 = torch.mm((1 - ytrue).view(1, -1), (1 - sigmoid).log())\n",
    "    return los_1 - los_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Estimating  parameters  in  logistic  regression.__ For this part you will use the follow four supplemental data files:\n",
    "    - __logistic\\_xtest.npy__: contains a 500-by-6 matrix whose rows are the independent variables (predictors) from the test set.\n",
    "    - __logistic\\_xtrain.npy__: contains a 2000-by-6 matrix whose rows are the independent variables (predictors) from the train set.\n",
    "    - __logistic\\_ytest.npy__: contains a binary 500-dimensional vector of dependent variables (responses) from the test set.\n",
    "    - __logistic\\_ytrain.npy__: contains a binary 2000-dimensional vector of dependent variables (responses) from the train set.\n",
    "    \n",
    "   The $i$-th row of the matrix in __logistic\\_xtrain.npy__ is the predictor for the response in the $i$-th entry of the vector in __logistic\\_ytrain.npy__, and analogously for the two test set files. Please include these files in your submission so that we can run your code without downloading them again. __Note:__ we didn't discuss reading numpy data from files. To load the files, you can simply call __xtrain = np.load('xtrain.npy')__ to read the data into the variable __xtrain__. __xtrain__ will be a Numpy array.\n",
    "\n",
    "   Load the training data and use it to obtain estimates of $W$ and $b$ by minimizing the negative log-likelihood via gradient descent. __Another note:__ you'll have to play around with the learning rate and the number of steps. Two good ways to check if optimization is finding a good minimizer:\n",
    "    - Try printing the training data loss before and after optimization.\n",
    "    - Use the test data to validate your estimated parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading training set\n",
    "logis_xtrain = np.load('logistic_xtrain.npy').astype(np.float32)\n",
    "logis_ytrain = np.load('logistic_ytrain.npy').astype(np.float32)\n",
    "# Loading test set\n",
    "logis_xtest = np.load('logistic_xtest.npy').astype(np.float32)\n",
    "logis_ytest = np.load('logistic_ytest.npy').astype(np.float32)\n",
    "# Converting from numpy\n",
    "logis_xtrain = torch.from_numpy(logis_xtrain)\n",
    "logis_ytrain = torch.from_numpy(logis_ytrain)\n",
    "logis_xtest = torch.from_numpy(logis_xtest)\n",
    "logis_ytest = torch.from_numpy(logis_ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 1327.836\n",
      "Loss = 1277.495\n",
      "Loss = 1136.209\n",
      "Loss = 1089.209\n",
      "Loss = 956.802\n",
      "Loss = 898.645\n",
      "Loss = 832.225\n",
      "Loss = 797.607\n",
      "Loss = 771.834\n",
      "Loss = 756.488\n",
      "Loss = 745.782\n",
      "Loss = 738.000\n",
      "Loss = 731.798\n",
      "Loss = 726.609\n",
      "Loss = 722.140\n",
      "Loss = 718.234\n",
      "Loss = 714.792\n",
      "Loss = 711.741\n",
      "Loss = 709.025\n",
      "Loss = 706.598\n",
      "Loss = 704.421\n",
      "Loss = 702.462\n",
      "Loss = 700.695\n",
      "Loss = 699.095\n",
      "Loss = 697.645\n",
      "Loss = 696.326\n",
      "Loss = 695.125\n",
      "Loss = 694.029\n",
      "Loss = 693.026\n",
      "Loss = 692.108\n",
      "Loss = 691.265\n",
      "Loss = 690.491\n",
      "Loss = 689.780\n",
      "Loss = 689.124\n",
      "Loss = 688.519\n",
      "Loss = 687.961\n",
      "Loss = 687.444\n",
      "Loss = 686.967\n",
      "Loss = 686.524\n",
      "Loss = 686.114\n",
      "Loss = 685.734\n",
      "Loss = 685.380\n",
      "Loss = 685.052\n",
      "Loss = 684.747\n",
      "Loss = 684.462\n",
      "Loss = 684.198\n",
      "Loss = 683.951\n",
      "Loss = 683.721\n",
      "Loss = 683.506\n",
      "Loss = 683.306\n",
      "Loss = 683.119\n",
      "Loss = 682.944\n",
      "Loss = 682.780\n",
      "Loss = 682.627\n",
      "Loss = 682.484\n",
      "Loss = 682.350\n",
      "Loss = 682.224\n",
      "Loss = 682.106\n",
      "Loss = 681.996\n",
      "Loss = 681.893\n",
      "Loss = 681.796\n",
      "Loss = 681.704\n",
      "Loss = 681.619\n",
      "Loss = 681.539\n",
      "Loss = 681.463\n",
      "Loss = 681.392\n",
      "Loss = 681.326\n",
      "Loss = 681.263\n",
      "Loss = 681.204\n",
      "Loss = 681.149\n",
      "Loss = 681.097\n",
      "Loss = 681.047\n",
      "Loss = 681.001\n",
      "Loss = 680.958\n",
      "Loss = 680.917\n",
      "Loss = 680.878\n",
      "Loss = 680.842\n",
      "Loss = 680.808\n",
      "Loss = 680.776\n",
      "Loss = 680.745\n",
      "Loss = 680.716\n",
      "Loss = 680.689\n",
      "Loss = 680.664\n",
      "Loss = 680.640\n",
      "Loss = 680.617\n",
      "Loss = 680.596\n",
      "Loss = 680.576\n",
      "Loss = 680.557\n",
      "Loss = 680.538\n",
      "Loss = 680.521\n",
      "Loss = 680.505\n",
      "Loss = 680.490\n",
      "Loss = 680.476\n",
      "Loss = 680.462\n",
      "Loss = 680.450\n",
      "Loss = 680.438\n",
      "Loss = 680.426\n",
      "Loss = 680.415\n",
      "Loss = 680.405\n",
      "Loss = 680.396\n",
      "Loss = 680.387\n",
      "Loss = 680.378\n",
      "Loss = 680.370\n",
      "Loss = 680.362\n",
      "Loss = 680.355\n",
      "Loss = 680.348\n",
      "Loss = 680.342\n",
      "Loss = 680.335\n",
      "Loss = 680.330\n",
      "Loss = 680.324\n",
      "Loss = 680.319\n",
      "Loss = 680.314\n",
      "Loss = 680.309\n",
      "Loss = 680.305\n",
      "Loss = 680.301\n",
      "Loss = 680.297\n",
      "Loss = 680.293\n",
      "Loss = 680.290\n",
      "Loss = 680.286\n",
      "Loss = 680.283\n",
      "Loss = 680.280\n",
      "Loss = 680.277\n",
      "Loss = 680.275\n",
      "Loss = 680.272\n",
      "Loss = 680.270\n",
      "Loss = 680.267\n",
      "Loss = 680.265\n",
      "Loss = 680.263\n",
      "Loss = 680.261\n",
      "Loss = 680.259\n",
      "Loss = 680.258\n",
      "Loss = 680.256\n",
      "Loss = 680.254\n",
      "Loss = 680.253\n",
      "Loss = 680.252\n",
      "Loss = 680.250\n",
      "Loss = 680.249\n",
      "Loss = 680.248\n",
      "Loss = 680.247\n",
      "Loss = 680.246\n",
      "Loss = 680.245\n",
      "Loss = 680.244\n",
      "Loss = 680.243\n",
      "Loss = 680.242\n",
      "Loss = 680.241\n",
      "Loss = 680.240\n",
      "Loss = 680.240\n",
      "Loss = 680.239\n",
      "Loss = 680.238\n",
      "Loss = 680.238\n",
      "Loss = 680.237\n",
      "Loss = 680.237\n",
      "Loss = 680.236\n",
      "Loss = 680.236\n",
      "Loss = 680.235\n",
      "Loss = 680.235\n",
      "Loss = 680.234\n",
      "Loss = 680.234\n",
      "Loss = 680.233\n",
      "Loss = 680.233\n",
      "Loss = 680.233\n",
      "Loss = 680.232\n",
      "Loss = 680.232\n",
      "Loss = 680.232\n",
      "Loss = 680.232\n",
      "Loss = 680.231\n",
      "Loss = 680.231\n",
      "Loss = 680.231\n",
      "Loss = 680.231\n",
      "Loss = 680.230\n",
      "Loss = 680.230\n",
      "Loss = 680.230\n",
      "Loss = 680.230\n",
      "Loss = 680.230\n",
      "Loss = 680.230\n",
      "Loss = 680.229\n",
      "Loss = 680.229\n",
      "Loss = 680.229\n",
      "Loss = 680.229\n",
      "Loss = 680.229\n",
      "Loss = 680.229\n",
      "Loss = 680.229\n",
      "Loss = 680.229\n",
      "Loss = 680.228\n",
      "Loss = 680.228\n",
      "Loss = 680.228\n",
      "Loss = 680.228\n",
      "Loss = 680.228\n",
      "Loss = 680.228\n",
      "Loss = 680.228\n",
      "Loss = 680.228\n",
      "Loss = 680.228\n",
      "Loss = 680.228\n",
      "Loss = 680.228\n",
      "Loss = 680.228\n",
      "Loss = 680.228\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.226\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.226\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.226\n",
      "Loss = 680.226\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.226\n",
      "Loss = 680.226\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.226\n",
      "Loss = 680.226\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.226\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.226\n",
      "Loss = 680.226\n",
      "Loss = 680.227\n",
      "Loss = 680.226\n",
      "Loss = 680.227\n",
      "Loss = 680.226\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.226\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.226\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.226\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n",
      "Loss = 680.227\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500  # No. of iteration\n",
    "lr = 0.005         # Learning Rate\n",
    "optimizer = torch.optim.SGD([w, b], lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss = nll_loss(logis_xtrain, w, b, logis_ytrain)\n",
    "    loss.backward()\n",
    "#     print(\"Loss = %.3f\" % (loss.item()))\n",
    "    optimizer.step()\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. __Evaluating logistic regression on test data.__ Load the test data. What is the negative log-likelihood of your model on this test data? That is, what is the negative log-likelihood when you use your estimated parameters with the previously unseen test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll_test = nll_loss(logis_xtest, w, b, logis_ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. __Evaluating the estimated logistic parameters.__ The data was, in reality, generated with \n",
    "$$ W = (1,1,2,3,5,8),~~~~~~ b = -1. $$\n",
    "    Write PyTorch expressions to compute the squared error between your estimated parameters and their true values. Evaluate the error in recovering $W$ and $b$ separately. What are the squared errors of these estimates? __Note:__ you need only evaluate the error of your final estimates, not at every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Values\n",
    "wtrue = torch.tensor([1, 1, 2, 3, 5, 8], dtype=torch.float32).view(-1, 1)\n",
    "btrue = torch.tensor([-1], dtype=torch.float32)\n",
    "\n",
    "W_squared_error = torch.mm((wtrue - w).view(1, -1), (wtrue - w))\n",
    "B_squared_error = torch.mm((btrue - b), (btrue - b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. For ease of grading, please make the variables from the above problems available in a dictionary called __results_logistic__. The dictionary should have keys __$W$, Wsqerr, $b$, bsqerr, log_lik_test__ corresponding to the evaluation of your estimate of $W$, $W$ squared error, $b$, $b$ squared error and the negative log-likelihood on the test set. For example, if my squared error for $W$ is stored in a torch.Tensor called __W_squared_error__, then the key __'Wsqerr'__ should have value __W_squared_error.item()__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W': array([[0.9780814],\n",
       "        [1.2326875],\n",
       "        [1.498215 ],\n",
       "        [3.0153086],\n",
       "        [4.6440253],\n",
       "        [7.5284047]], dtype=float32),\n",
       " 'Wsqerr': 0.6557665467262268,\n",
       " 'b': array([[-0.95491993]], dtype=float32),\n",
       " 'bsqerr': 0.0020322122145444155,\n",
       " 'log_lik_test': 163.01419067382812}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_logistic = {\"W\": w.detach().numpy(),\n",
    "                    \"Wsqerr\": W_squared_error.item(),\n",
    "                    \"b\": b.detach().numpy(),\n",
    "                    \"bsqerr\": B_squared_error.item(),\n",
    "                    \"log_lik_test\": nll_test.item()}\n",
    "results_logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. __Classification  of  normally  distributed  data.__ For this part you will use four Numpy __.npy__ files that contain train and test data generated from $K=3$ different classes. Each class $k \\in \\{1,2,3\\}$ has an associated mean $\\mu_k \\in R$ and variance $\\sigma^2_k \\in R$, and all observations from a given class are i.i.d. $\\mathcal{N}(\\mu_k,\\sigma^2_k)$. The four files are:\n",
    "    - __normal_xtest.npy__: contains a 500-vector whose entries are the independent variables (predictors) from the test set.\n",
    "    - __normal_xtrain.npy__: contains a 2000-vector whose entries are the independent variables (predictors) from the train set.\n",
    "    - __normal_ytest.npy__: contains a 500-by-3 dimensional matrix whose rows are one-hot encodings of the class labels for the test set.\n",
    "    - __normal_ytrain.npy__: contains a 2000-by-3 dimensional matrix whose rows are one-hot encodings of the class labels for the train set.\n",
    "    \n",
    "   The $i$-th entry of the vector in __normal_xtrain.npy__ is the observed random variable from class with label given by the $i$-th row of the matrix in __normal_ytrain.npy__, and analogously for the two test set files. Please include these files in your submission so that we can run your code without downloading them again.\n",
    "\n",
    "   Load the training data and use it to obtain estimates of the vector of class means $\\mu = (\\mu_0,\\mu_1,\\mu_2)$ and variances $\\sigma^2 = (\\sigma^2_0, \\sigma^2_1, \\sigma^2_2)$ by minimizing the cross-entropy between the estimated normals and the one-hot encodings of the class labels (as we did in our softmax regression example in class). Please name the corresponding variables __mu__ and __sigma2__. This time, instead of using gradient descent, use Adagrad, supplied by Pytorch as the function __torch.optim.Adagrad__. Adagrad is a _stochastic gradient descent algorithm_, popular in machine learning. You can call this just like the gradient descent optimizer we used in class---just supply a learning rate. Documentation for the PyTorch implementation of Adagrad can be found here: https://pytorch.org/docs/stable/_modules/torch/optim/adagrad.html. See https://en.wikipedia.org/wiki/Stochastic_gradient_descent for more information about stochastic gradient descent and the Adagrad algorithm.\n",
    "\t\n",
    "   __Note:__ you'll no longer be able to use the built-in logit cross-entropy that we used for training our models in lecture. Your cross-entropy for one observation should now look something like $ -\\sum_k y'_k \\log p_k,$ where $y'$ is the one-hot encoded vector and $p$ is the vector whose $k$-th entry is the (estimated) probability of the $k$-th observation given its class. __Another note:__ do not include any estimation of the mixing coefficients (i.e., the class priors) in your model. You only need to estimate three means and three variances, because we are building a _discriminative_ model in this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading training set\n",
    "normal_xtrain = np.load('normal_xtrain.npy').astype(np.float32)\n",
    "normal_ytrain = np.load('normal_ytrain.npy').astype(np.float32)\n",
    "# Loading test set\n",
    "normal_xtest = np.load('normal_xtest.npy').astype(np.float32)\n",
    "normal_ytest = np.load('normal_ytest.npy').astype(np.float32)\n",
    "# Converting from numpy\n",
    "normal_xtrain = torch.from_numpy(normal_xtrain)\n",
    "normal_ytrain = torch.from_numpy(normal_ytrain)\n",
    "normal_xtest = torch.from_numpy(normal_xtest)\n",
    "normal_ytest = torch.from_numpy(normal_ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 14691.7139 Correct: 0.05\n",
      "Epoch: 1 Loss: 8218.1787 Correct: 0.49\n",
      "Epoch: 2 Loss: 1708.9786 Correct: 0.67\n",
      "Epoch: 3 Loss: 1597.4380 Correct: 0.69\n",
      "Epoch: 4 Loss: 1521.5144 Correct: 0.69\n",
      "Epoch: 5 Loss: 1464.4652 Correct: 0.69\n",
      "Epoch: 6 Loss: 1419.7297 Correct: 0.69\n",
      "Epoch: 7 Loss: 1384.2609 Correct: 0.69\n",
      "Epoch: 8 Loss: 1356.1569 Correct: 0.69\n",
      "Epoch: 9 Loss: 1333.6637 Correct: 0.69\n",
      "Epoch: 10 Loss: 1315.3507 Correct: 0.69\n",
      "Epoch: 11 Loss: 1300.1851 Correct: 0.68\n",
      "Epoch: 12 Loss: 1287.4419 Correct: 0.68\n",
      "Epoch: 13 Loss: 1276.6058 Correct: 0.68\n",
      "Epoch: 14 Loss: 1267.2993 Correct: 0.68\n",
      "Epoch: 15 Loss: 1259.2415 Correct: 0.68\n",
      "Epoch: 16 Loss: 1252.2156 Correct: 0.68\n",
      "Epoch: 17 Loss: 1246.0532 Correct: 0.68\n",
      "Epoch: 18 Loss: 1240.6205 Correct: 0.68\n",
      "Epoch: 19 Loss: 1235.8093 Correct: 0.67\n",
      "Epoch: 20 Loss: 1231.5311 Correct: 0.67\n",
      "Epoch: 21 Loss: 1227.7129 Correct: 0.67\n",
      "Epoch: 22 Loss: 1224.2936 Correct: 0.67\n",
      "Epoch: 23 Loss: 1221.2207 Correct: 0.67\n",
      "Epoch: 24 Loss: 1218.4498 Correct: 0.67\n",
      "Epoch: 25 Loss: 1215.9420 Correct: 0.67\n",
      "Epoch: 26 Loss: 1213.6631 Correct: 0.67\n",
      "Epoch: 27 Loss: 1211.5826 Correct: 0.67\n",
      "Epoch: 28 Loss: 1209.6737 Correct: 0.67\n",
      "Epoch: 29 Loss: 1207.9126 Correct: 0.67\n",
      "Epoch: 30 Loss: 1206.2775 Correct: 0.67\n",
      "Epoch: 31 Loss: 1204.7491 Correct: 0.67\n",
      "Epoch: 32 Loss: 1203.3112 Correct: 0.67\n",
      "Epoch: 33 Loss: 1201.9493 Correct: 0.67\n",
      "Epoch: 34 Loss: 1200.6511 Correct: 0.67\n",
      "Epoch: 35 Loss: 1199.4064 Correct: 0.67\n",
      "Epoch: 36 Loss: 1198.2059 Correct: 0.67\n",
      "Epoch: 37 Loss: 1197.0433 Correct: 0.67\n",
      "Epoch: 38 Loss: 1195.9122 Correct: 0.68\n",
      "Epoch: 39 Loss: 1194.8079 Correct: 0.68\n",
      "Epoch: 40 Loss: 1193.7261 Correct: 0.68\n",
      "Epoch: 41 Loss: 1192.6635 Correct: 0.68\n",
      "Epoch: 42 Loss: 1191.6176 Correct: 0.68\n",
      "Epoch: 43 Loss: 1190.5859 Correct: 0.68\n",
      "Epoch: 44 Loss: 1189.5667 Correct: 0.68\n",
      "Epoch: 45 Loss: 1188.5574 Correct: 0.68\n",
      "Epoch: 46 Loss: 1187.5574 Correct: 0.68\n",
      "Epoch: 47 Loss: 1186.5649 Correct: 0.68\n",
      "Epoch: 48 Loss: 1185.5792 Correct: 0.68\n",
      "Epoch: 49 Loss: 1184.5989 Correct: 0.68\n",
      "Epoch: 50 Loss: 1183.6233 Correct: 0.68\n",
      "Epoch: 51 Loss: 1182.6514 Correct: 0.69\n",
      "Epoch: 52 Loss: 1181.6827 Correct: 0.69\n",
      "Epoch: 53 Loss: 1180.7168 Correct: 0.69\n",
      "Epoch: 54 Loss: 1179.7526 Correct: 0.69\n",
      "Epoch: 55 Loss: 1178.7897 Correct: 0.69\n",
      "Epoch: 56 Loss: 1177.8281 Correct: 0.69\n",
      "Epoch: 57 Loss: 1176.8672 Correct: 0.69\n",
      "Epoch: 58 Loss: 1175.9062 Correct: 0.69\n",
      "Epoch: 59 Loss: 1174.9453 Correct: 0.69\n",
      "Epoch: 60 Loss: 1173.9845 Correct: 0.69\n",
      "Epoch: 61 Loss: 1173.0228 Correct: 0.69\n",
      "Epoch: 62 Loss: 1172.0605 Correct: 0.69\n",
      "Epoch: 63 Loss: 1171.0973 Correct: 0.69\n",
      "Epoch: 64 Loss: 1170.1333 Correct: 0.69\n",
      "Epoch: 65 Loss: 1169.1682 Correct: 0.69\n",
      "Epoch: 66 Loss: 1168.2014 Correct: 0.69\n",
      "Epoch: 67 Loss: 1167.2339 Correct: 0.69\n",
      "Epoch: 68 Loss: 1166.2648 Correct: 0.69\n",
      "Epoch: 69 Loss: 1165.2946 Correct: 0.69\n",
      "Epoch: 70 Loss: 1164.3229 Correct: 0.70\n",
      "Epoch: 71 Loss: 1163.3499 Correct: 0.70\n",
      "Epoch: 72 Loss: 1162.3756 Correct: 0.70\n",
      "Epoch: 73 Loss: 1161.4000 Correct: 0.70\n",
      "Epoch: 74 Loss: 1160.4231 Correct: 0.70\n",
      "Epoch: 75 Loss: 1159.4454 Correct: 0.70\n",
      "Epoch: 76 Loss: 1158.4666 Correct: 0.70\n",
      "Epoch: 77 Loss: 1157.4867 Correct: 0.70\n",
      "Epoch: 78 Loss: 1156.5061 Correct: 0.70\n",
      "Epoch: 79 Loss: 1155.5250 Correct: 0.70\n",
      "Epoch: 80 Loss: 1154.5437 Correct: 0.70\n",
      "Epoch: 81 Loss: 1153.5616 Correct: 0.70\n",
      "Epoch: 82 Loss: 1152.5795 Correct: 0.70\n",
      "Epoch: 83 Loss: 1151.5970 Correct: 0.70\n",
      "Epoch: 84 Loss: 1150.6151 Correct: 0.70\n",
      "Epoch: 85 Loss: 1149.6335 Correct: 0.70\n",
      "Epoch: 86 Loss: 1148.6523 Correct: 0.70\n",
      "Epoch: 87 Loss: 1147.6719 Correct: 0.70\n",
      "Epoch: 88 Loss: 1146.6925 Correct: 0.70\n",
      "Epoch: 89 Loss: 1145.7142 Correct: 0.70\n",
      "Epoch: 90 Loss: 1144.7374 Correct: 0.70\n",
      "Epoch: 91 Loss: 1143.7625 Correct: 0.70\n",
      "Epoch: 92 Loss: 1142.7893 Correct: 0.71\n",
      "Epoch: 93 Loss: 1141.8182 Correct: 0.71\n",
      "Epoch: 94 Loss: 1140.8496 Correct: 0.71\n",
      "Epoch: 95 Loss: 1139.8837 Correct: 0.71\n",
      "Epoch: 96 Loss: 1138.9207 Correct: 0.71\n",
      "Epoch: 97 Loss: 1137.9609 Correct: 0.71\n",
      "Epoch: 98 Loss: 1137.0043 Correct: 0.71\n",
      "Epoch: 99 Loss: 1136.0516 Correct: 0.71\n",
      "Epoch: 100 Loss: 1135.1031 Correct: 0.71\n",
      "Epoch: 101 Loss: 1134.1583 Correct: 0.71\n",
      "Epoch: 102 Loss: 1133.2184 Correct: 0.71\n",
      "Epoch: 103 Loss: 1132.2831 Correct: 0.71\n",
      "Epoch: 104 Loss: 1131.3529 Correct: 0.71\n",
      "Epoch: 105 Loss: 1130.4277 Correct: 0.72\n",
      "Epoch: 106 Loss: 1129.5082 Correct: 0.72\n",
      "Epoch: 107 Loss: 1128.5947 Correct: 0.71\n",
      "Epoch: 108 Loss: 1127.6868 Correct: 0.72\n",
      "Epoch: 109 Loss: 1126.7854 Correct: 0.72\n",
      "Epoch: 110 Loss: 1125.8903 Correct: 0.72\n",
      "Epoch: 111 Loss: 1125.0022 Correct: 0.71\n",
      "Epoch: 112 Loss: 1124.1207 Correct: 0.71\n",
      "Epoch: 113 Loss: 1123.2463 Correct: 0.72\n",
      "Epoch: 114 Loss: 1122.3795 Correct: 0.72\n",
      "Epoch: 115 Loss: 1121.5204 Correct: 0.72\n",
      "Epoch: 116 Loss: 1120.6691 Correct: 0.72\n",
      "Epoch: 117 Loss: 1119.8257 Correct: 0.72\n",
      "Epoch: 118 Loss: 1118.9904 Correct: 0.72\n",
      "Epoch: 119 Loss: 1118.1633 Correct: 0.72\n",
      "Epoch: 120 Loss: 1117.3447 Correct: 0.72\n",
      "Epoch: 121 Loss: 1116.5348 Correct: 0.72\n",
      "Epoch: 122 Loss: 1115.7339 Correct: 0.72\n",
      "Epoch: 123 Loss: 1114.9414 Correct: 0.72\n",
      "Epoch: 124 Loss: 1114.1582 Correct: 0.72\n",
      "Epoch: 125 Loss: 1113.3842 Correct: 0.72\n",
      "Epoch: 126 Loss: 1112.6194 Correct: 0.72\n",
      "Epoch: 127 Loss: 1111.8638 Correct: 0.72\n",
      "Epoch: 128 Loss: 1111.1176 Correct: 0.72\n",
      "Epoch: 129 Loss: 1110.3809 Correct: 0.72\n",
      "Epoch: 130 Loss: 1109.6536 Correct: 0.72\n",
      "Epoch: 131 Loss: 1108.9360 Correct: 0.73\n",
      "Epoch: 132 Loss: 1108.2281 Correct: 0.73\n",
      "Epoch: 133 Loss: 1107.5297 Correct: 0.73\n",
      "Epoch: 134 Loss: 1106.8411 Correct: 0.73\n",
      "Epoch: 135 Loss: 1106.1619 Correct: 0.73\n",
      "Epoch: 136 Loss: 1105.4927 Correct: 0.72\n",
      "Epoch: 137 Loss: 1104.8329 Correct: 0.72\n",
      "Epoch: 138 Loss: 1104.1829 Correct: 0.73\n",
      "Epoch: 139 Loss: 1103.5424 Correct: 0.73\n",
      "Epoch: 140 Loss: 1102.9114 Correct: 0.73\n",
      "Epoch: 141 Loss: 1102.2902 Correct: 0.73\n",
      "Epoch: 142 Loss: 1101.6782 Correct: 0.73\n",
      "Epoch: 143 Loss: 1101.0757 Correct: 0.73\n",
      "Epoch: 144 Loss: 1100.4825 Correct: 0.73\n",
      "Epoch: 145 Loss: 1099.8989 Correct: 0.73\n",
      "Epoch: 146 Loss: 1099.3245 Correct: 0.73\n",
      "Epoch: 147 Loss: 1098.7592 Correct: 0.73\n",
      "Epoch: 148 Loss: 1098.2028 Correct: 0.73\n",
      "Epoch: 149 Loss: 1097.6555 Correct: 0.73\n",
      "Epoch: 150 Loss: 1097.1171 Correct: 0.73\n",
      "Epoch: 151 Loss: 1096.5875 Correct: 0.73\n",
      "Epoch: 152 Loss: 1096.0668 Correct: 0.73\n",
      "Epoch: 153 Loss: 1095.5544 Correct: 0.73\n",
      "Epoch: 154 Loss: 1095.0503 Correct: 0.73\n",
      "Epoch: 155 Loss: 1094.5548 Correct: 0.73\n",
      "Epoch: 156 Loss: 1094.0674 Correct: 0.73\n",
      "Epoch: 157 Loss: 1093.5883 Correct: 0.73\n",
      "Epoch: 158 Loss: 1093.1171 Correct: 0.73\n",
      "Epoch: 159 Loss: 1092.6539 Correct: 0.73\n",
      "Epoch: 160 Loss: 1092.1984 Correct: 0.73\n",
      "Epoch: 161 Loss: 1091.7505 Correct: 0.73\n",
      "Epoch: 162 Loss: 1091.3102 Correct: 0.73\n",
      "Epoch: 163 Loss: 1090.8771 Correct: 0.73\n",
      "Epoch: 164 Loss: 1090.4518 Correct: 0.73\n",
      "Epoch: 165 Loss: 1090.0333 Correct: 0.74\n",
      "Epoch: 166 Loss: 1089.6217 Correct: 0.74\n",
      "Epoch: 167 Loss: 1089.2170 Correct: 0.74\n",
      "Epoch: 168 Loss: 1088.8196 Correct: 0.74\n",
      "Epoch: 169 Loss: 1088.4286 Correct: 0.74\n",
      "Epoch: 170 Loss: 1088.0439 Correct: 0.74\n",
      "Epoch: 171 Loss: 1087.6660 Correct: 0.74\n",
      "Epoch: 172 Loss: 1087.2941 Correct: 0.74\n",
      "Epoch: 173 Loss: 1086.9290 Correct: 0.74\n",
      "Epoch: 174 Loss: 1086.5695 Correct: 0.74\n",
      "Epoch: 175 Loss: 1086.2161 Correct: 0.74\n",
      "Epoch: 176 Loss: 1085.8687 Correct: 0.74\n",
      "Epoch: 177 Loss: 1085.5269 Correct: 0.74\n",
      "Epoch: 178 Loss: 1085.1907 Correct: 0.74\n",
      "Epoch: 179 Loss: 1084.8601 Correct: 0.74\n",
      "Epoch: 180 Loss: 1084.5352 Correct: 0.74\n",
      "Epoch: 181 Loss: 1084.2152 Correct: 0.74\n",
      "Epoch: 182 Loss: 1083.9009 Correct: 0.74\n",
      "Epoch: 183 Loss: 1083.5914 Correct: 0.74\n",
      "Epoch: 184 Loss: 1083.2872 Correct: 0.74\n",
      "Epoch: 185 Loss: 1082.9877 Correct: 0.74\n",
      "Epoch: 186 Loss: 1082.6932 Correct: 0.74\n",
      "Epoch: 187 Loss: 1082.4034 Correct: 0.74\n",
      "Epoch: 188 Loss: 1082.1183 Correct: 0.74\n",
      "Epoch: 189 Loss: 1081.8378 Correct: 0.74\n",
      "Epoch: 190 Loss: 1081.5616 Correct: 0.74\n",
      "Epoch: 191 Loss: 1081.2899 Correct: 0.74\n",
      "Epoch: 192 Loss: 1081.0227 Correct: 0.74\n",
      "Epoch: 193 Loss: 1080.7596 Correct: 0.74\n",
      "Epoch: 194 Loss: 1080.5007 Correct: 0.74\n",
      "Epoch: 195 Loss: 1080.2460 Correct: 0.74\n",
      "Epoch: 196 Loss: 1079.9954 Correct: 0.74\n",
      "Epoch: 197 Loss: 1079.7483 Correct: 0.74\n",
      "Epoch: 198 Loss: 1079.5052 Correct: 0.74\n",
      "Epoch: 199 Loss: 1079.2662 Correct: 0.74\n",
      "Epoch: 200 Loss: 1079.0308 Correct: 0.74\n",
      "Epoch: 201 Loss: 1078.7987 Correct: 0.74\n",
      "Epoch: 202 Loss: 1078.5706 Correct: 0.74\n",
      "Epoch: 203 Loss: 1078.3458 Correct: 0.74\n",
      "Epoch: 204 Loss: 1078.1245 Correct: 0.74\n",
      "Epoch: 205 Loss: 1077.9065 Correct: 0.74\n",
      "Epoch: 206 Loss: 1077.6919 Correct: 0.74\n",
      "Epoch: 207 Loss: 1077.4806 Correct: 0.74\n",
      "Epoch: 208 Loss: 1077.2727 Correct: 0.74\n",
      "Epoch: 209 Loss: 1077.0674 Correct: 0.74\n",
      "Epoch: 210 Loss: 1076.8658 Correct: 0.74\n",
      "Epoch: 211 Loss: 1076.6670 Correct: 0.74\n",
      "Epoch: 212 Loss: 1076.4712 Correct: 0.74\n",
      "Epoch: 213 Loss: 1076.2781 Correct: 0.74\n",
      "Epoch: 214 Loss: 1076.0884 Correct: 0.74\n",
      "Epoch: 215 Loss: 1075.9011 Correct: 0.74\n",
      "Epoch: 216 Loss: 1075.7167 Correct: 0.74\n",
      "Epoch: 217 Loss: 1075.5350 Correct: 0.74\n",
      "Epoch: 218 Loss: 1075.3560 Correct: 0.74\n",
      "Epoch: 219 Loss: 1075.1796 Correct: 0.74\n",
      "Epoch: 220 Loss: 1075.0060 Correct: 0.74\n",
      "Epoch: 221 Loss: 1074.8347 Correct: 0.74\n",
      "Epoch: 222 Loss: 1074.6660 Correct: 0.74\n",
      "Epoch: 223 Loss: 1074.4996 Correct: 0.74\n",
      "Epoch: 224 Loss: 1074.3358 Correct: 0.74\n",
      "Epoch: 225 Loss: 1074.1744 Correct: 0.74\n",
      "Epoch: 226 Loss: 1074.0153 Correct: 0.74\n",
      "Epoch: 227 Loss: 1073.8584 Correct: 0.74\n",
      "Epoch: 228 Loss: 1073.7041 Correct: 0.74\n",
      "Epoch: 229 Loss: 1073.5516 Correct: 0.74\n",
      "Epoch: 230 Loss: 1073.4015 Correct: 0.74\n",
      "Epoch: 231 Loss: 1073.2534 Correct: 0.74\n",
      "Epoch: 232 Loss: 1073.1077 Correct: 0.74\n",
      "Epoch: 233 Loss: 1072.9637 Correct: 0.74\n",
      "Epoch: 234 Loss: 1072.8223 Correct: 0.74\n",
      "Epoch: 235 Loss: 1072.6823 Correct: 0.74\n",
      "Epoch: 236 Loss: 1072.5447 Correct: 0.74\n",
      "Epoch: 237 Loss: 1072.4088 Correct: 0.74\n",
      "Epoch: 238 Loss: 1072.2750 Correct: 0.74\n",
      "Epoch: 239 Loss: 1072.1429 Correct: 0.74\n",
      "Epoch: 240 Loss: 1072.0128 Correct: 0.74\n",
      "Epoch: 241 Loss: 1071.8845 Correct: 0.74\n",
      "Epoch: 242 Loss: 1071.7583 Correct: 0.74\n",
      "Epoch: 243 Loss: 1071.6334 Correct: 0.74\n",
      "Epoch: 244 Loss: 1071.5105 Correct: 0.74\n",
      "Epoch: 245 Loss: 1071.3893 Correct: 0.74\n",
      "Epoch: 246 Loss: 1071.2694 Correct: 0.74\n",
      "Epoch: 247 Loss: 1071.1516 Correct: 0.74\n",
      "Epoch: 248 Loss: 1071.0353 Correct: 0.74\n",
      "Epoch: 249 Loss: 1070.9208 Correct: 0.74\n",
      "Epoch: 250 Loss: 1070.8075 Correct: 0.74\n",
      "Epoch: 251 Loss: 1070.6959 Correct: 0.74\n",
      "Epoch: 252 Loss: 1070.5858 Correct: 0.74\n",
      "Epoch: 253 Loss: 1070.4773 Correct: 0.74\n",
      "Epoch: 254 Loss: 1070.3702 Correct: 0.74\n",
      "Epoch: 255 Loss: 1070.2648 Correct: 0.74\n",
      "Epoch: 256 Loss: 1070.1606 Correct: 0.74\n",
      "Epoch: 257 Loss: 1070.0577 Correct: 0.74\n",
      "Epoch: 258 Loss: 1069.9564 Correct: 0.74\n",
      "Epoch: 259 Loss: 1069.8564 Correct: 0.75\n",
      "Epoch: 260 Loss: 1069.7581 Correct: 0.75\n",
      "Epoch: 261 Loss: 1069.6605 Correct: 0.75\n",
      "Epoch: 262 Loss: 1069.5647 Correct: 0.74\n",
      "Epoch: 263 Loss: 1069.4700 Correct: 0.74\n",
      "Epoch: 264 Loss: 1069.3766 Correct: 0.74\n",
      "Epoch: 265 Loss: 1069.2845 Correct: 0.74\n",
      "Epoch: 266 Loss: 1069.1935 Correct: 0.74\n",
      "Epoch: 267 Loss: 1069.1039 Correct: 0.74\n",
      "Epoch: 268 Loss: 1069.0153 Correct: 0.74\n",
      "Epoch: 269 Loss: 1068.9281 Correct: 0.74\n",
      "Epoch: 270 Loss: 1068.8419 Correct: 0.74\n",
      "Epoch: 271 Loss: 1068.7571 Correct: 0.74\n",
      "Epoch: 272 Loss: 1068.6730 Correct: 0.74\n",
      "Epoch: 273 Loss: 1068.5905 Correct: 0.74\n",
      "Epoch: 274 Loss: 1068.5087 Correct: 0.74\n",
      "Epoch: 275 Loss: 1068.4282 Correct: 0.74\n",
      "Epoch: 276 Loss: 1068.3485 Correct: 0.74\n",
      "Epoch: 277 Loss: 1068.2701 Correct: 0.74\n",
      "Epoch: 278 Loss: 1068.1927 Correct: 0.75\n",
      "Epoch: 279 Loss: 1068.1162 Correct: 0.74\n",
      "Epoch: 280 Loss: 1068.0410 Correct: 0.74\n",
      "Epoch: 281 Loss: 1067.9663 Correct: 0.74\n",
      "Epoch: 282 Loss: 1067.8931 Correct: 0.74\n",
      "Epoch: 283 Loss: 1067.8204 Correct: 0.74\n",
      "Epoch: 284 Loss: 1067.7488 Correct: 0.75\n",
      "Epoch: 285 Loss: 1067.6783 Correct: 0.75\n",
      "Epoch: 286 Loss: 1067.6084 Correct: 0.75\n",
      "Epoch: 287 Loss: 1067.5397 Correct: 0.75\n",
      "Epoch: 288 Loss: 1067.4719 Correct: 0.75\n",
      "Epoch: 289 Loss: 1067.4045 Correct: 0.75\n",
      "Epoch: 290 Loss: 1067.3385 Correct: 0.75\n",
      "Epoch: 291 Loss: 1067.2732 Correct: 0.75\n",
      "Epoch: 292 Loss: 1067.2087 Correct: 0.75\n",
      "Epoch: 293 Loss: 1067.1451 Correct: 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 294 Loss: 1067.0822 Correct: 0.75\n",
      "Epoch: 295 Loss: 1067.0201 Correct: 0.75\n",
      "Epoch: 296 Loss: 1066.9589 Correct: 0.75\n",
      "Epoch: 297 Loss: 1066.8983 Correct: 0.75\n",
      "Epoch: 298 Loss: 1066.8386 Correct: 0.75\n",
      "Epoch: 299 Loss: 1066.7799 Correct: 0.75\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "torch.manual_seed(1)\n",
    "num_class = 3\n",
    "mu = torch.rand(1, num_class, dtype=torch.float32, requires_grad=True)\n",
    "sigma = torch.rand(1, num_class, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model train\n",
    "lr2 = 0.5\n",
    "num_epochs_2 = 300\n",
    "optimizer_2 = torch.optim.Adagrad([mu, sigma], lr=lr2)\n",
    "\n",
    "\n",
    "def log_prob(x, mu, sigma):\n",
    "    pdfs = torch.distributions.normal.Normal(loc=mu, scale=sigma)\n",
    "    probs = pdfs.log_prob(x)  # Compute log probs at x, for three normals\n",
    "    m = torch.nn.Softmax(dim=1)  # Softmax to mutually exclusice classes\n",
    "    return m(probs)\n",
    "\n",
    "\n",
    "def cross_entropy(output, target):\n",
    "    delta = 1e-8  # Avoid log(0)\n",
    "    return -torch.sum(target*torch.log(output + delta))\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs_2):\n",
    "    output = log_prob(normal_xtrain, mu, sigma)\n",
    "#     print(output)\n",
    "    loss = cross_entropy(output, normal_ytrain)\n",
    "    loss.backward()\n",
    "#     print(loss)\n",
    "    optimizer_2.step()\n",
    "    mu.grad.zero_()\n",
    "    sigma.grad.zero_()\n",
    "\n",
    "#     prediction = output.max(1).indices\n",
    "#     correct = 0\n",
    "#     correct += (prediction == normal_ytrain.max(1).indices).sum().item()\n",
    "\n",
    "#     print(\"Epoch: {} Loss: {:.4f} Correct: {:.2f}\".format(\n",
    "#         epoch,\n",
    "#         loss.item(),\n",
    "#         correct / len(normal_ytrain)\n",
    "#     ))\n",
    "\n",
    "sigma2 = sigma**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. __Evaluating loss on test data.__ Load the test data. What is the cross-entropy of your model on this test data? That is, what is the cross-entropy when you use your estimated parameters with the previously unseen test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test = log_prob(normal_xtest, mu, sigma)\n",
    "loss_test = cross_entropy(output_test, normal_ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. __Evaluating parameter estimation on test data.__ The true parameter values for the three classes were\n",
    "\\begin{equation*} \\begin{aligned}\n",
    "\\mu_0 &= -1, \\sigma_0^2 = 0.5 \\\\\n",
    "\\mu_1 &= 0, \\sigma_1^2 = 1 \\\\\n",
    "\\mu_2 &= 3, \\sigma_2^2 = 1.5.\n",
    "\\end{aligned} \n",
    "\\end{equation*}\n",
    "   Write a PyTorch expression to compute the total squared error (i.e., summed over the six parameters) between your estimates and their true values. What is the squared error? __Note:__ you need only evaluate the error of your final estimates, not at every step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06522228568792343 0.14182530343532562\n"
     ]
    }
   ],
   "source": [
    "mutrue = torch.tensor([-1, 0, 3], dtype=torch.float32).view(-1, 1)\n",
    "sigma2true = torch.tensor([0.5, 1, 1.5], dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "Mu_squared_error = torch.mm((mutrue - mu.view(-1, 1)).view(1, -1),\n",
    "                            (mutrue - mu.view(-1, 1)))\n",
    "\n",
    "Sigma2_squared_error = torch.mm((sigma2true - sigma2.view(-1, 1)).view(1, -1),\n",
    "                                (sigma2true - sigma2.view(-1, 1)))\n",
    "\n",
    "print(Mu_squared_error.item(), Sigma2_squared_error.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. __Evaluating classification error on test data.__ Write and evaluate a PyTorch expression that computes the classification error of your estimated model averaged over the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test = log_prob(normal_xtest, mu, sigma)\n",
    "\n",
    "prediction = output_test.max(1).indices\n",
    "correct = 0\n",
    "correct += (prediction == normal_ytest.max(1).indices).sum().item()\n",
    "\n",
    "class_error = (1 - correct / len(normal_ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Again, for ease of grading, define a dictionary called __results_class__, with keys __'mu', 'sigma2', 'crossent_test', 'class_error'__ with keys corresponding to the evaluation of your estimate of $\\mu$, $\\sigma^2$, the cross-entropy on the test set, and the classification error from the previous problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mu': array([[-1.2394042 , -0.05149129,  2.9274979 ]], dtype=float32), 'sigma2': array([[0.6580336, 1.1745735, 1.2061042]], dtype=float32), \"crossent_test'\": 268.8582763671875, 'class_error': 0.268}\n"
     ]
    }
   ],
   "source": [
    "results_class = {\"mu\": mu.detach().numpy(),\n",
    "                 \"sigma2\": sigma2.detach().numpy(),\n",
    "                 \"crossent_test'\": loss_test.item(),\n",
    "                 \"class_error\": class_error}\n",
    "print(results_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Constructing $pandas$ objects (2 points)\n",
    "In this problem, you will create two simple $pandas$ objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a $pandas$ Series object with indices given by the first 10 letters of the English alphabet and values given by the first 10 primes. Assign this object to a variable called __alphaprimes__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def primes(p=2):\n",
    "    while True:\n",
    "        while any(p % x == 0 for x in range(2, p)):\n",
    "            p += 1\n",
    "        yield p\n",
    "        p += 1\n",
    "\n",
    "\n",
    "prime = primes()\n",
    "primes = [next(prime) for _ in range(10)]\n",
    "idx = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j']\n",
    "\n",
    "alphaprimes = pd.Series(primes, index=idx)"
   ]
  },
  {
   "attachments": {
    "table.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Below is a table that might arise in a genetics experiment. Reconstruct this as a $pandas$ DataFrame and assign it to a variable called __animals__.\n",
    "\n",
    "![table.png](attachment:table.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal = [\"goat\"]*4 + [\"bird\"]*4 + [\"llama\"]*4\n",
    "parent1 = [\"A\", \"A\", \"a\", \"a\"]*3\n",
    "parent2 = [\"A\", \"a\"]*6\n",
    "scores = {\n",
    "    \"score1\": [i for i in range(1, 13)],\n",
    "    \"score2\": [2, 4, 4, 6, 6, 8, 8, 10, 10, 12, 12, 14]\n",
    "}\n",
    "\n",
    "arrs = [animal, parent1, parent2]\n",
    "index = pd.MultiIndex.from_arrays(arrs, names=[\"animal\", \"parent1\", \"parent2\"])\n",
    "\n",
    "animals = pd.DataFrame(scores, index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Working with $pandas$ DataFrames (5 points)\n",
    "In this problem, you'll get practice working with $pandas$ DataFrames, reading them into and out of memory, changing their contents and performing aggregation operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read __iris.csv__ into Python as a $pandas$ DataFrame. Note that the CSV file includes column headers. How many data points are there in this data set? What are the data types of the columns? What are the column names? The column names correspond to flower species names, as well as four basic measurements one can make of a flower: the width and length of its petals and the width and length of its sepal (the part of the pant that supports and protects the flower itself). How many species of flower are included in the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:  \n",
    "data points : 150  \n",
    "data types : float64(4), object(1)  \n",
    "column names : Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, Species  \n",
    "number of species: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Sepal.Length  150 non-null    float64\n",
      " 1   Sepal.Width   150 non-null    float64\n",
      " 2   Petal.Length  150 non-null    float64\n",
      " 3   Petal.Width   150 non-null    float64\n",
      " 4   Species       150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df = pd.read_csv(\"iris.csv\")\n",
    "# data points, column types, column names\n",
    "iris_df.info()\n",
    "# species\n",
    "iris_df[\"Species\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The iris data is based on the data initially uploaded to the UC Irvine machine learning repository. It is now known that this data contains errors in two of its rows (see the documentation at https://archive.ics.uci.edu/ml/datasets/Iris). Using 1-indexing, these errors are in the 35th and 38th rows. The 35th row should read __4.9,3.1,1.5,0.2,\"setosa\"__, where the fourth feature is incorrect as it appears in the file, and the 38th row should read __4.9,3.6,1.4,0.1,\"setosa\"__, where the second and third features are incorrect as they appear in the file. Correct these entries of your DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.iloc[34, 3] = 0.2\n",
    "iris_df.iloc[37, 1] = 3.6\n",
    "iris_df.iloc[37, 2] = 1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The iris dataset is commonly used in machine learning as a proving ground for clustering and classification algorithms. Some researchers have found it useful to use two additional features, called _Petal ratio_ and _Sepal ratio_, defined as the ratio of the petal length to petal width and the ratio of the sepal length to sepal width, respectively. Add two columns to your DataFrame corresponding to these two new features. Name these columns __Petal.Ratio__ and __Sepal.Ratio__, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df[\"Petal.Ratio\"] = iris_df[\"Petal.Length\"] / iris_df[\"Petal.Width\"]\n",
    "iris_df[\"Sepal.Ratio\"] = iris_df[\"Sepal.Length\"] / iris_df[\"Sepal.Width\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Save your corrected and extended DataFrame to a csv file called __iris_corrected.csv__. (This file does not need to be included in your submission.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.to_csv(\"iris_corrected.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Use a $pandas$ aggregate operation to determine the mean, median, minimum, maximum and standard deviation of the petal and sepal ratio for each of the three species in the data set. __Note:__ you should be able to get all five numbers in a single table (indeed, in a single line of code) using a well-chosen group-by or aggregate operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"5\" halign=\"left\">Petal.Ratio</th>\n",
       "      <th colspan=\"5\" halign=\"left\">Sepal.Ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Species</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>setosa</th>\n",
       "      <td>6.908000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.854545</td>\n",
       "      <td>1.470188</td>\n",
       "      <td>1.463063</td>\n",
       "      <td>1.268293</td>\n",
       "      <td>1.956522</td>\n",
       "      <td>0.118750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>versicolor</th>\n",
       "      <td>3.242837</td>\n",
       "      <td>3.240385</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.312456</td>\n",
       "      <td>2.160402</td>\n",
       "      <td>2.161290</td>\n",
       "      <td>1.764706</td>\n",
       "      <td>2.818182</td>\n",
       "      <td>0.228658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>virginica</th>\n",
       "      <td>2.780662</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>2.125000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.407367</td>\n",
       "      <td>2.230453</td>\n",
       "      <td>2.169540</td>\n",
       "      <td>1.823529</td>\n",
       "      <td>2.961538</td>\n",
       "      <td>0.246992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Petal.Ratio                                     Sepal.Ratio  \\\n",
       "                  mean    median       min   max       std        mean   \n",
       "Species                                                                  \n",
       "setosa        6.908000  7.000000  2.666667  15.0  2.854545    1.470188   \n",
       "versicolor    3.242837  3.240385  2.666667   4.1  0.312456    2.160402   \n",
       "virginica     2.780662  2.666667  2.125000   4.0  0.407367    2.230453   \n",
       "\n",
       "                                                    \n",
       "              median       min       max       std  \n",
       "Species                                             \n",
       "setosa      1.463063  1.268293  1.956522  0.118750  \n",
       "versicolor  2.161290  1.764706  2.818182  0.228658  \n",
       "virginica   2.169540  1.823529  2.961538  0.246992  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_df.groupby(\"Species\")[[\"Petal.Ratio\",\n",
    "                            \"Sepal.Ratio\"]].agg(['mean', 'median',\n",
    "                                                 'min', 'max', 'std'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Plotting Dataframes: Major League Baseball (8 points)\n",
    "\n",
    "\n",
    "__Requisite legal boilerplate:__ The information used here was obtained free of charge from and is copyrighted by Retrosheet.  Interested parties may contact [Retrosheet](www.retrosheet.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the data into a table called __mlb_df__. Each row of the table represents the outcome of a single game from the 2018 MLB season. Take note that the file does not have columns names; see the __header__ keyword to the __pandas.read_csv__ function. The columns are explained in a __.txt__ file which you can download from https://www.retrosheet.org/gamelogs/glfields.txt, but we will only make use of a few of them in this problem. The 10-th and 11-th columns (using 1-indexing) are the scores of the visiting and home teams, respectively. Rename these columns __v_score__ and __h_score__, respectively. MLB comprises two leagues, the American League and the National League, encoded as __AL__ and __NL__ in the table. The 5-th and 8-th columns (also 1-indexed) are the league affiliations of the visiting and home team, respectively. Rename these columns __v_league__ and __h_league__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb_df = pd.read_csv(\"GL2018.TXT\", header=None)\n",
    "mlb_df.rename(columns={4: 'v_league', 7: 'h_league',\n",
    "                       9: 'v_score', 10: 'h_score'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a plot with two subplots, placed side-by-side. Each subplot should be a scatter plot in which the x- and y-axes correspond to the home and visitor scores, respectively, and in which each point corresponds to a game from the season. In the left-hand plot, include all games in which both teams were in the NL, and in the right-hand plot, include all games in which both teams were in the AL. Games in which the teams were from different leagues should be ignored. Specify the transparency (cf. the __alpha__ parameter in the __matplotlib__ documentation) so that scores that occur more often will be shaded darker than rare scores. Color the points in the scatter plot according to the league affiliation of the two teams as follows: games between two teams both in the AL should be rendered as red points in the scatter plot. Games between two teams both in the NL should be rendered as blue points in the scatter plot. Label your axes and provide an appropriate title for your plot as well as its subplots. Save your plot in a pdf file called __home_vs_away_scores.pdf__. This file does not need to be including in your submission. __Note:__ you may find it useful to create an extra column in the encoding whether a given game is AL vs AL, NL vs NL or mixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cleaning df\n",
    "plot_df = mlb_df[['v_league', 'h_league', 'v_score', 'h_score']]\n",
    "plot_df = plot_df[plot_df.v_league == plot_df.h_league]\n",
    "plot_df.loc[plot_df['v_league'] == 'NL', 'Color'] = 'blue'\n",
    "plot_df.loc[plot_df['v_league'] == 'AL', 'Color'] = 'red'\n",
    "\n",
    "# Aesthetic\n",
    "fig1 = plt.figure(1, figsize=(13, 8))\n",
    "plot_colors = ['blue', 'red']\n",
    "# Generating Plot\n",
    "for i in range(len(plot_colors)):\n",
    "    plt.subplot(1, len(plot_colors), i+1)\n",
    "    plt.scatter(x=plot_df.loc[plot_df['Color'] == plot_colors[i]]['h_score'],\n",
    "                y=plot_df.loc[plot_df['Color'] == plot_colors[i]]['v_score'],\n",
    "                s=10, c=plot_colors[i], marker='o', alpha=0.3)\n",
    "    plt.xlabel('h_score')\n",
    "    plt.ylabel('v_score')\n",
    "fig1.savefig('home_vs_away_scores.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The Skellam distribution https://en.wikipedia.org/wiki/Skellam_distribution) is the distribution that results from taking the difference between two Poisson random variables. It is often suggested as a model for the difference between scores in sports games, particularly baseball. Add a new column to the data frame called __score_diff__, given by the home score minus the away score. Make a histogram of this score difference and give the plot an appropriate title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlb_df['score_diff'] = mlb_df.h_score - mlb_df.v_score\n",
    "\n",
    "# Aesthetic\n",
    "fig2 = plt.figure(2, figsize=(13, 8))\n",
    "# Generating Plot\n",
    "plt.hist(x=mlb_df['score_diff'], bins=30)\n",
    "plt.xlabel('difference')\n",
    "plt.ylabel('frequency')\n",
    "plt.title('Difference between Home and Away Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Read the documentation about the __scipy__ implementation of the Skellam distribution at https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.skellam.html . If $\\lambda_H$ and $\\lambda_V$ are the means of two independent Poisson random variables $K_H$ and $K_V$, respectively, then the Skellam distribution that describes the difference $K_H - K_V$ has parameters $\\lambda_H$ and $\\lambda_V$. We will assume (perhaps incorrectly) that the correlation and location parameters of the Skellam distribution are both $0$. To fit a Skellam distribution to the data, we will first fit Poisson distributions to the home and away teams. Estimate parameters $\\hat{\\lambda}_H$ and $\\hat{\\lambda}_V$ as the means of the home and visitor scores, respectively. Save your estimates in variables __lambda_home__ and __lambda_visitor__, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_home = mlb_df.h_score.mean()\n",
    "lambda_visitor = mlb_df.v_score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Now let's run a goodness-of-fit test to see how well the Skellam distribution matches our data. There are several ways to do this, but we will use the $\\chi^2$ test, which is available in __scipy__ as __scipy.stats.chisquare__ (documentation here: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chisquare.html ). There are two problems we must address. First, the Skellam distribution predicts that sometimes $K_H = K_V$, which cannot happen in our data because ties are not allowed in baseball (play continues until a team wins https://en.wikipedia.org/wiki/Longest_professional_baseball_game), so our model should really be the Skellam distribution conditioned on the fact that $K_H \\neq K_V$. Second, the $\\chi^2$ test as implemented in __scipy__ requires that the observed and expected frequencies be finite-length arrays, but the Skellam distribution gives non-zero probability to every possible integer value. To account for these two issues, we will model the score difference as a Skellam distribution conditioned on the event that $K_H \\neq K_V$ and $|K_H-K_V| \\le 30$ ($36$ is the most runs ever scored by one team in a baseball game, so $30$ is a reasonable upper bound for our purposes). Use the __scipy__ implementation of the $\\chi^2$ test to assess the fit of our model to the data (use the default delta degrees of freedom, __ddof=0__). Save the resulting p-value and the test statistic in variables named __chi2pvalue__ and __chi2stat__, respectively. Does our model agree with the data? What changes might we consider to improve the fit of our model (a few sentences will suffice here--- no need to implement any of these improvements unless you really want to)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:  \n",
    "The resulting p-value is extremely small, and thus we will reject the null hypothesis that there is no significant difference between the observed and estimated distributions. Therefore, we can NOT say this is a good fit.  \n",
    "To improve the fit, we may estimate and take into account the correlation and location parameters to improve our estimates of the parameters $\\hat{\\lambda}_H$, $\\hat{\\lambda}_V$. For example, we can estimate the correlation parameter $\\rho$ by using the two observed distributions, `v_score` and `h_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9127607560227973e-87\n"
     ]
    }
   ],
   "source": [
    "# Observed Freq\n",
    "observed_freq = pd.Series(mlb_df['score_diff'].value_counts())\n",
    "\n",
    "# Estimating Probs\n",
    "estimated_probs = {i: stats.skellam.pmf(k=i, mu1=lambda_home,\n",
    "                                        mu2=lambda_visitor, loc=0)\n",
    "                   for i in range(-30, 0)}\n",
    "estimated_probs.update({i: stats.skellam.pmf(k=i, mu1=lambda_home,\n",
    "                                             mu2=lambda_visitor, loc=0)\n",
    "                        for i in range(1, 31)})\n",
    "\n",
    "# Normalizing\n",
    "normalize_c = 1 / sum(estimated_probs.values())\n",
    "for k, v in estimated_probs.items():\n",
    "    estimated_probs[k] = v * normalize_c\n",
    "\n",
    "# Estimating Freqs\n",
    "estimated_freq = {}\n",
    "for k, v in estimated_probs.items():\n",
    "    estimated_freq[k] = round(v * len(mlb_df['score_diff']))\n",
    "estimated_freq = pd.Series(estimated_freq)\n",
    "\n",
    "# Trim values < 5\n",
    "freq_df = pd.concat([observed_freq, estimated_freq],\n",
    "                    axis=1, names=['observed', 'estimated'])\n",
    "freq_df.rename(columns={'score_diff': 'obs', 0: 'est'}, inplace=True)\n",
    "trimmed_df = freq_df.loc[freq_df.obs >= 5].loc[freq_df.est >= 5]\n",
    "\n",
    "# Chi Squared Test\n",
    "chi2stat, chi2pvalue = stats.chisquare(f_obs=trimmed_df['obs'].to_numpy(),\n",
    "                                       f_exp=trimmed_df['est'].to_numpy(),\n",
    "                                       ddof=0)\n",
    "\n",
    "print(chi2pvalue)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
